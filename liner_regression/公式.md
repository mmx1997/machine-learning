# 线性回归

## 原理推导

由输入$x$和输出$y$构成的数据集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$，其中$x_i=(x_{i1};x_{i2};...;x_{id}),y\in R$。

线性回归学习的关键问题在于确定参数$w$和$b$，使得拟合输出$y$与真实输出$y_i$尽可能接近。在回归中，通常使用均方误差衡量预测与真实标签之间的损失，所以回归的优化目标就是使均方误差最小化，所以有：

$(w^*,b^*)&= \ arg min \sum_{i=1}^{m}(y-y_i)^2\hfill \\ &=\ arg min \sum_{i=1}^{m}(wx_i+b-y_i)^2$

对上式求一阶偏导并令其为0，对$w$的推导如下式：

$\frac{\part L(w,b)}{\part w} &= \frac{\part}{\part w} \begin{bmatrix} \sum_{i=1}^{m}(wx_i+b-y_i)^2 \end{bmatrix} \hfill \\ &= \sum_{i=1}^{m}\frac{\part}{\part m}[(y_i-wx_i-b)^2] \hfill \\ &=\sum_{i=1}^{m}[2 * (y_i-wx_i-b)*(-x_i) ] \hfill \\ &=\sum_{i=1}^{m}[2*(wx_i^2-y_ix_i+bx_i)]\hfill \\ &=2*(w\sum_{i=1}^{m}x_i^2-\sum_{i=1}^{m}y_ix_i+b\sum_{i=1}^{m}x_i)$

同理，对$b$求导有：

$\frac{\part L(w,b)}{\part b}&=\frac{\part}{\part b}[\sum_{i=1}^{m}(wx_i+b-y_i)^2]\hfill \\ &=\sum_{i=1}^{m}\frac{\part}{\part b}[(y_i-wx_i-b)^2]\hfill \\ &=\sum_{i=1}^{m}[2*(y-wx_i-b)*(-1)]\hfill \\ &=2*(-\sum_{i=1}^{m}y_i+\sum_{i=1}^{m}wx_i+\sum_{i=1}^{m}b)\hfill \\ &=2*(mb-\sum_{i=1}^{m}(y-wx_i))\hfill$

分别令其为0，可解的$w$和$b$的最优表达式为：

$w^*=\frac{\sum_{i=1}^{m}y_i(x_i-\hat{x})}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2}$

$b^*=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)$

为了适应批次化训练，将$w$和$b$合并为$\hat{w}=(w;b)$，训练数据$D$表示为一个$m\times d$维的矩阵$X$，表示为：

$X=\begin{bmatrix} x_{11} & ... & x_{1d} \\ x_{21} & ... & x_{2d} \\ ... & ... & ... \\ x_{m1} & ... & x_{md} \end{bmatrix} = \begin{bmatrix} x_1^T \\ x_2^T \\ ... \\ x_m^T  \end{bmatrix}$

最小化问题变为：$\hat{w}^*=arg min(y-X\hat{w})^T(y-X\hat{w})$

令$L=(y-X\hat{w})^T(y-X\hat{w})$，对$\hat{w}$求导，推导过程为：

$L=y^Ty-y^TX\hat w -\hat wX^Ty+\hat wX^TX\hat w$

$\frac{\part L}{\part \hat w}=\frac {\part y^T y}{\part \hat w}-\frac{\part y^TX\hat w}{\part \hat w}-\frac{\part \hat wX^Ty}{\part \hat w}+\frac{\part \hat wX^TX\hat w}{\part \hat w}$